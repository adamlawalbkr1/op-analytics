{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of materialized view names\n",
    "mvs = [\n",
    "    {'mv_name': 'across_bridging_txs_v3', 'start_date': '2024-07-01'},\n",
    "    # {'mv_name': 'across_bridging_txs_v3_logs_only', 'start_date': '2024-07-01'},\n",
    "    # {'mv_name': 'filtered_logs_l2s', 'start_date': ''},\n",
    "    ### {'mv_name': 'erc20_transfers', 'start_date': ''},\n",
    "    ### {'mv_name': 'native_eth_transfers', 'start_date': ''},\n",
    "    ### {'mv_name': 'transactions_unique', 'start_date': ''},\n",
    "    # {'mv_name': 'daily_aggregate_transactions_to', 'start_date': ''},\n",
    "    {'mv_name': 'event_emitting_transactions_l2s', 'start_date': ''},\n",
    "    \n",
    "]\n",
    "\n",
    "set_days_batch_size = 30\n",
    "\n",
    "optimize_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import traceback\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "import opstack_metadata_utils as ops\n",
    "import goldsky_db_utils as gsb\n",
    "sys.path.pop()\n",
    "client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_names = [item['mv_name'] for item in mvs]\n",
    "\n",
    "# Get Chain List\n",
    "chain_configs = ops.get_superchain_metadata_by_data_source('oplabs') # OPLabs db\n",
    "\n",
    "if client is None:\n",
    "        client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "# Function to create ClickHouse view\n",
    "def get_chain_names_from_df(df):\n",
    "    return df['blockchain'].dropna().unique().tolist()\n",
    "\n",
    "# chain_configs = chain_configs[chain_configs['chain_name'] == 'xterio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-18\n"
     ]
    }
   ],
   "source": [
    "# List of chains\n",
    "# chains = get_chain_names_from_df(chain_configs)\n",
    "\n",
    "# Start date for backfilling\n",
    "start_date = datetime.date(2021, 11, 1)\n",
    "# start_date = datetime.date(2024, 5, 1)\n",
    "end_date = datetime.date.today() + datetime.timedelta(days=1)\n",
    "\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_from_file(mv_name):\n",
    "    try:\n",
    "        # Try to get the directory of the current script\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # If __file__ is not defined (e.g., in Jupyter), use the current working directory\n",
    "        script_dir = os.getcwd()\n",
    "    \n",
    "    query_file_path = os.path.join(script_dir, 'mv_inputs', f'{mv_name}.sql')\n",
    "    # print(f\"Attempting to read query from: {query_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(query_file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Query file not found: {query_file_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimize_on_insert(option_int = 1):\n",
    "    client.command(f\"\"\"\n",
    "        SET optimize_on_insert = {option_int};\n",
    "        \"\"\")\n",
    "    print(f\"Set optimize_on_insert = {option_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import clickhouse_connect\n",
    "from clickhouse_connect.driver.exceptions import ClickHouseError\n",
    "\n",
    "def create_materialized_view(client, chain, mv_name, block_time = 2):\n",
    "    table_view_name = f'{chain}_{mv_name}'\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    create_file_name = f'{mv_name}_create'\n",
    "    print(full_view_name)\n",
    "    \n",
    "    # Check if create file exists\n",
    "    if not os.path.exists(f'mv_inputs/{create_file_name}.sql'):\n",
    "        print(f\"Table create file {create_file_name}.sql does not exist. Skipping table creation.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Check if table already exists\n",
    "            result = client.query(f\"SHOW TABLES LIKE '{table_view_name}'\")\n",
    "            result_rows = list(result.result_rows)\n",
    "            if result_rows:\n",
    "                print(f\"Table {table_view_name} already exists. Skipping creation.\")\n",
    "            else:\n",
    "                # Create the table\n",
    "                create_query = get_query_from_file(create_file_name)\n",
    "                create_query = create_query.format(chain=chain, view_name=table_view_name)\n",
    "                client.command(create_query)\n",
    "                print(f\"Created table {table_view_name}\")\n",
    "        except ClickHouseError as e:\n",
    "            print(f\"Error creating table {table_view_name}: {str(e)}\")\n",
    "            return  # Exit the function if table creation fails\n",
    "\n",
    "    # Comment out matview since we're backfilling daily now\n",
    "    # try:\n",
    "    #     # Check if view already exists\n",
    "    #     result = client.query(f\"SHOW TABLES LIKE '{full_view_name}'\")\n",
    "    #     result_rows = list(result.result_rows)\n",
    "    #     if result_rows:\n",
    "    #         print(f\"Materialized view {full_view_name} already exists. Skipping creation.\")\n",
    "    #         return\n",
    "\n",
    "    #     query_template = get_query_from_file(f'{mv_name}_mv')\n",
    "    #     query = query_template.format(chain=chain, view_name=full_view_name, table_name=table_view_name, block_time_sec=block_time)\n",
    "    #     query = gsb.process_goldsky_sql(query)\n",
    "    #     # Save the query\n",
    "    #     output_folder = os.path.join(\"mv_outputs\", \"sql\")\n",
    "    #     os.makedirs(output_folder, exist_ok=True)\n",
    "    #     filename = f\"{mv_name}_mv.sql\"\n",
    "    #     file_path = os.path.join(output_folder, filename)\n",
    "    #     with open(file_path, 'w') as file:\n",
    "    #         file.write(query)\n",
    "    #     # print(query)\n",
    "    #     client.command(query)\n",
    "    #     print(f\"Created materialized view {full_view_name}\")\n",
    "    # except ClickHouseError as e:\n",
    "    #     print(f\"Error creating materialized view {full_view_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "def ensure_backfill_tracking_table_exists(client):\n",
    "    check_table_query = \"\"\"\n",
    "    SELECT 1 FROM system.tables \n",
    "    WHERE database = currentDatabase() AND name = 'backfill_tracking'\n",
    "    \"\"\"\n",
    "    result = client.query(check_table_query)\n",
    "    \n",
    "    if not result.result_rows:\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE backfill_tracking (\n",
    "            chain String,\n",
    "            mv_name String,\n",
    "            start_date Date,\n",
    "            end_date Date\n",
    "        ) ENGINE = MergeTree()\n",
    "        ORDER BY (chain, mv_name, start_date)\n",
    "        \"\"\"\n",
    "        client.command(create_table_query)\n",
    "        print(\"Created backfill_tracking table.\")\n",
    "    else:\n",
    "        print(\"backfill_tracking table already exists.\")\n",
    "\n",
    "def backfill_data(client, chain, mv_name, end_date = end_date, block_time = 2, mod_start_date = start_date):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    full_table_name = f'{chain}_{mv_name}'\n",
    "    if mod_start_date == '':\n",
    "        current_date_q = f\"SELECT DATE_TRUNC('day',MIN(timestamp)) AS start_dt FROM {chain}_blocks WHERE number = 1 AND is_deleted = 0\"\n",
    "        current_date = client.query(current_date_q).result_rows[0][0].date()\n",
    "    else:\n",
    "        current_date = pd.to_datetime(mod_start_date).date()\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        print(f\"{chain} - {mv_name}: Current date: {current_date} - End Date: {end_date}\")\n",
    "        attempts = 1\n",
    "        is_success = 0\n",
    "        days_batch_size = set_days_batch_size\n",
    "        \n",
    "        while (is_success == 0) & (attempts < 3) :#& (current_date + datetime.timedelta(days=days_batch_size) <= end_date):\n",
    "            batch_size = datetime.timedelta(days=days_batch_size)\n",
    "            print(f\"attempt: {attempts}\")\n",
    "            batch_end = min(current_date + batch_size, end_date)\n",
    "            # print(f'init batch end: {batch_end}')\n",
    "            # print('checking backfill tracking')\n",
    "            # Check if this range has been backfilled\n",
    "            check_query = f\"\"\"\n",
    "            SELECT MAX(start_date) AS latest_fill_start\n",
    "            FROM backfill_tracking\n",
    "            WHERE chain = '{chain}'\n",
    "            AND mv_name = '{mv_name}'\n",
    "            HAVING \n",
    "                MIN(start_date) <= toDate('{current_date}')\n",
    "            AND MAX(end_date) > toDate('{batch_end}')\n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "            result = client.query(check_query)\n",
    "\n",
    "            if result.result_rows: # Get date to start backfilling\n",
    "                latest_fill_start = result.result_rows[0][0]\n",
    "                # print(f\"Latest Fill Result: {latest_fill_start}\")\n",
    "                current_date = max(latest_fill_start, current_date)\n",
    "                batch_end = min(current_date + batch_size, end_date + datetime.timedelta(days = 1))\n",
    "            else:\n",
    "                print(\"no backfill exists\")\n",
    "            \n",
    "            # print(f'backfill check batch end: {batch_end}')\n",
    "            # print(f\"Fill start: {current_date}\")\n",
    "\n",
    "            # print(check_query)\n",
    "            # print(result.result_rows)\n",
    "            #Check if data already exists\n",
    "\n",
    "            # Start 1 day back\n",
    "            query_start_date = current_date - datetime.timedelta(days = 1)\n",
    "            query_end_date = batch_end #+ datetime.timedelta(days = 1)\n",
    "\n",
    "\n",
    "            if not result.result_rows:\n",
    "                # No record of backfill, proceed\n",
    "                query_template = get_query_from_file(f'{mv_name}_backfill')\n",
    "                query = query_template.format(\n",
    "                    view_name=full_view_name,\n",
    "                    chain=chain,\n",
    "                    start_date=query_start_date,\n",
    "                    end_date=query_end_date,\n",
    "                    table_name = full_table_name,\n",
    "                    block_time_sec = block_time\n",
    "                )\n",
    "                query = gsb.process_goldsky_sql(query)\n",
    "                \n",
    "                # print(query)\n",
    "                try:\n",
    "                    # print(query)\n",
    "                    # set_optimize_on_insert(0) # for runtime\n",
    "                    print(f\"Starting backfill for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "\n",
    "                    # # Save the query\n",
    "                    # output_folder = os.path.join(\"mv_outputs\", \"sql\")\n",
    "                    # os.makedirs(output_folder, exist_ok=True)\n",
    "                    # filename = f\"{chain}_{mv_name}_backfill.sql\"\n",
    "                    # file_path = os.path.join(output_folder, filename)\n",
    "                    # with open(file_path, 'w') as file:\n",
    "                    #     file.write(query)\n",
    "\n",
    "                    client.command(query)\n",
    "                    # Record the backfill\n",
    "                    track_query = f\"\"\"\n",
    "                    INSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\n",
    "                    VALUES ('{chain}', '{mv_name}', toDate('{current_date}'), toDate('{batch_end}'))\n",
    "                    \"\"\"\n",
    "                    client.command(track_query)\n",
    "                    \n",
    "                    print(f\"Backfilled data for {full_view_name} from {current_date} to {batch_end}\")\n",
    "\n",
    "                    # Optimize the newly backfilled partition\n",
    "                    # optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "                    is_success = 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during backfill for {full_view_name} from {current_date} to {batch_end}: {str(e)}\")\n",
    "                    days_batch_size = 1\n",
    "                    attempts += 1\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print(f\"Data already backfilled for {full_view_name} from {current_date} to {batch_end}. Skipping.\")\n",
    "                is_success = 1\n",
    "                # if optimize_all:\n",
    "                #     optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "        # print(f\"Current Date: {current_date}, Batch End: {batch_end}\")\n",
    "        current_date = max(batch_end,current_date + datetime.timedelta(days=1))\n",
    "\n",
    "        # print(f\"New Current Date: {current_date}\")\n",
    "\n",
    "# def optimize_partition(client, full_view_name, start_date, end_date):\n",
    "#     # First, let's get the actual partition names\n",
    "#     partition_query = f\"\"\"\n",
    "#     SELECT DISTINCT partition\n",
    "#     FROM system.parts\n",
    "#     WHERE table = '{full_view_name.split('.')[-1]}'\n",
    "#       AND database = '{full_view_name.split('.')[0]}'\n",
    "#       AND active\n",
    "#     ORDER BY partition\n",
    "#     \"\"\"\n",
    "#     print(partition_query)\n",
    "#     partitions = [row[0] for row in client.query(partition_query).result_rows]\n",
    "    \n",
    "#     print(f\"Available partitions for {full_view_name}: {partitions}\")\n",
    "\n",
    "#     # Filter partitions within our date range\n",
    "#     start_partition = start_date.strftime('%Y%m')\n",
    "#     end_partition = end_date.strftime('%Y%m')\n",
    "#     partitions_to_optimize = [p for p in partitions if start_partition <= p <= end_partition]\n",
    "\n",
    "#     if not partitions_to_optimize:\n",
    "#         print(f\"No partitions found for {full_view_name} between {start_date} and {end_date}\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         for partition in partitions_to_optimize:\n",
    "#             optimize_query = f\"\"\"\n",
    "#             OPTIMIZE TABLE {full_view_name} \n",
    "#             PARTITION '{partition}'\n",
    "#             FINAL SETTINGS max_execution_time = 3000\n",
    "#             \"\"\"\n",
    "#             print(f\"Attempting to optimize {full_view_name} for partition {partition}\")\n",
    "#             client.command(optimize_query)\n",
    "#             print(f\"Successfully optimized partition {partition} for {full_view_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing OPTIMIZE TABLE for {full_view_name}\")\n",
    "#         print(f\"  Partition: {partition}\")\n",
    "#         print(f\"  Date range: {start_date} to {end_date}\")\n",
    "#         print(f\"  Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_reset_materialized_view(client, chain, mv_name):\n",
    "    table_view_name = f'{chain}_{mv_name}'\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    create_file_name = f'{mv_name}_create'\n",
    "    print(full_view_name)\n",
    "    query_template = get_query_from_file(f'{mv_name}_mv')\n",
    "    query = query_template.format(chain=chain, view_name=full_view_name, table_name=table_view_name, block_time_sec=block_time)\n",
    "    query = gsb.process_goldsky_sql(query)\n",
    "    client.command(query)\n",
    "    print(f\"Updated materialized view {full_view_name}\")\n",
    "\n",
    "    # dt_cmd = f\"DETACH TABLE PERMANENTLY {full_view_name}\"\n",
    "    # dt_cmd = f\"ALTER TABLE {full_view_name} MODIFY QUERY SELECT 1 WHERE 0\"\n",
    "\n",
    "    # print(dt_cmd)\n",
    "    client.command(dt_cmd)\n",
    "    print(f\"Detached table {full_view_name}\")\n",
    "\n",
    "def reset_materialized_view(client, chain, mv_name,):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    table_name = f'{chain}_{mv_name}'\n",
    "\n",
    "    try:\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        # client.command(f\"DROP MATERIALIZED VIEW IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped materialized view {full_view_name}\")\n",
    "\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped table {table_name}\")\n",
    "\n",
    "        # Clear the backfill tracking for this view\n",
    "        bf_delete = f\"\"\"\n",
    "        ALTER TABLE backfill_tracking \n",
    "        DELETE WHERE chain = '{chain}' AND mv_name = '{mv_name}'\n",
    "        \"\"\"\n",
    "        # print(bf_delete)\n",
    "        client.command(bf_delete)\n",
    "\n",
    "        print(f\"Cleared backfill tracking for {full_view_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error resetting materialized view {full_view_name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # To reset a view\n",
    "for row in chain_configs.itertuples(index=False):\n",
    "        chain = row.chain_name\n",
    "        reset_materialized_view(client, chain, 'across_bridging_txs_v3')\n",
    "\n",
    "# # # # # # reset a single chain\n",
    "# # # # reset_materialized_view(client, 'xterio', 'daily_aggregate_transactions_to')\n",
    "\n",
    "# # # # # # # # # # # for mv in mv_names:\n",
    "# # # # # # # # # # #         # print(row)\n",
    "# # # # # # # # # # #         reset_materialized_view(client, 'bob', mv)\n",
    "\n",
    "\n",
    "# # # # # # # Clear all\n",
    "# # # # # # # mv_names\n",
    "# # # # # # # for row in chain_configs.itertuples(index=False):\n",
    "# # # # # # #         for mv in mv_names:\n",
    "# # # # # # #                 chain = row.chain_name\n",
    "# # # # # # #                 reset_materialized_view(client, chain, mv)\n",
    "\n",
    "# # Detach all\n",
    "# detach_reset_materialized_view\n",
    "# for row in chain_configs.itertuples(index=False):\n",
    "#         for mv in mv_names:\n",
    "#                 chain = row.chain_name\n",
    "#                 detach_reset_materialized_view(client, chain, mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backfill_tracking table already exists.\n",
      "Processing chain: op - across_bridging_txs_v3\n",
      "create matview\n",
      "op_across_bridging_txs_v3_mv\n",
      "Table op_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "op - across_bridging_txs_v3: Current date: 2024-07-01 - End Date: 2024-09-18\n",
      "attempt: 1\n",
      "no backfill exists\n",
      "Starting backfill for op_across_bridging_txs_v3_mv from 2024-06-30 to 2024-07-31\n",
      "Error during backfill for op_across_bridging_txs_v3_mv from 2024-07-01 to 2024-07-31: :HTTPDriver for https://pdmv9lhojy.us-west-2.aws.clickhouse.cloud:8443 returned response code 400)\n",
      " Code: 41. DB::Exception: Cannot parse datetime: Cannot parse DateTime from String: while converting source column dst_chain to destination column insert_time: while executing 'FUNCTION _CAST(dst_chain :: 20, DateTime :: 29) -> _CAST(dst_chain, DateTime) DateTime : 28'. (CANNOT_PARSE_DATETIME) (version 24.5.1.22957 (official build))\n",
      "\n",
      "attempt: 2\n",
      "no backfill exists\n",
      "Starting backfill for op_across_bridging_txs_v3_mv from 2024-06-30 to 2024-07-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate backfill\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mbackfill_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mblock_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod_start_date\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmod_start_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn error occurred:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 159\u001b[0m, in \u001b[0;36mbackfill_data\u001b[0;34m(client, chain, mv_name, end_date, block_time, mod_start_date)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting backfill for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_view_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_start_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# # Save the query\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# output_folder = os.path.join(\"mv_outputs\", \"sql\")\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# os.makedirs(output_folder, exist_ok=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# with open(file_path, 'w') as file:\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m#     file.write(query)\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Record the backfill\u001b[39;00m\n\u001b[1;32m    161\u001b[0m track_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124mINSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124mVALUES (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, toDate(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), toDate(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m))\u001b[39m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/clickhouse_connect/driver/httpclient.py:337\u001b[0m, in \u001b[0;36mHttpClient.command\u001b[0;34m(self, cmd, parameters, data, settings, use_database, external_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_settings(settings \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m    336\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m payload \u001b[38;5;129;01mor\u001b[39;00m fields \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 337\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/clickhouse_connect/driver/httpclient.py:418\u001b[0m, in \u001b[0;36mHttpClient._raw_request\u001b[0;34m(self, data, params, headers, method, retries, stream, server_wait, fields, error_handler)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_session \u001b[38;5;241m=\u001b[39m query_session\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ex\u001b[38;5;241m.\u001b[39m__context__, \u001b[38;5;167;01mConnectionResetError\u001b[39;00m):\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;66;03m# The server closed the connection, probably because the Keep Alive has expired\u001b[39;00m\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;66;03m# We should be safe to retry, as ClickHouse should not have processed anything on a connection\u001b[39;00m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;66;03m# that it killed.  We also only retry this once, as multiple disconnects are unlikely to be\u001b[39;00m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;66;03m# related to the Keep Alive settings\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/urllib3/request.py:81\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     78\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/urllib3/request.py:173\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    170\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[1;32m    171\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gcp-env/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "ensure_backfill_tracking_table_exists(client)\n",
    "\n",
    "for mv_row in mvs:\n",
    "\n",
    "    for chain_row in chain_configs.itertuples(index=False):\n",
    "        chain = chain_row.chain_name\n",
    "        block_time = chain_row.block_time_sec\n",
    "\n",
    "        mv_name = mv_row['mv_name']\n",
    "        print(f\"Processing chain: {chain} - {mv_name}\")\n",
    "\n",
    "        if mv_row['start_date'] != '':\n",
    "            mod_start_date = mv_row['start_date']\n",
    "        else:\n",
    "            mod_start_date = start_date\n",
    "        \n",
    "        try:\n",
    "            print('create matview')\n",
    "            create_materialized_view(client, chain, mv_name, block_time = block_time)\n",
    "        except:\n",
    "            print('error')\n",
    "        try:\n",
    "            print('create backfill')\n",
    "            backfill_data(client, chain, mv_name, end_date = end_date, block_time = block_time, mod_start_date = mod_start_date)\n",
    "        except Exception as e:\n",
    "            print('An error occurred:')\n",
    "            print(str(e))\n",
    "            print('Traceback:')\n",
    "            print(traceback.format_exc())\n",
    "        \n",
    "    print(f\"Completed processing for {chain}\")\n",
    "\n",
    "print(\"All chains and views processed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
