{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of materialized view names\n",
    "mv_names = [\n",
    "        'erc20_transfers',\n",
    "        # 'erc721_transfers',\n",
    "        'eth_transfers',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "import opstack_metadata_utils as ops\n",
    "sys.path.pop()\n",
    "client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Chain List\n",
    "chain_configs = ops.get_superchain_metadata_by_data_source('oplabs') # OPLabs db\n",
    "# Start date for backfilling\n",
    "start_date = datetime.date(2021, 1, 1)\n",
    "end_date = datetime.date.today()\n",
    "\n",
    "if client is None:\n",
    "        client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "# Function to create ClickHouse view\n",
    "def get_chain_names_from_df(df):\n",
    "    return df['blockchain'].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of chains\n",
    "chains = get_chain_names_from_df(chain_configs)\n",
    "\n",
    "# Start date for backfilling\n",
    "start_date = datetime.date(2021, 1, 1)\n",
    "end_date = datetime.date.today() + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_from_file(mv_name):\n",
    "    try:\n",
    "        # Try to get the directory of the current script\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # If __file__ is not defined (e.g., in Jupyter), use the current working directory\n",
    "        script_dir = os.getcwd()\n",
    "    \n",
    "    query_file_path = os.path.join(script_dir, 'mv_inputs', f'{mv_name}.sql')\n",
    "    print(f\"Attempting to read query from: {query_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(query_file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Query file not found: {query_file_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_materialized_view(client, chain, mv_name):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    \n",
    "    # Check if view already exists\n",
    "    result = client.query(f\"SHOW TABLES LIKE '{full_view_name}'\")\n",
    "    if result.result_rows:\n",
    "        print(f\"Materialized view {full_view_name} already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    query_template = get_query_from_file(f'{mv_name}_mv')\n",
    "    query = query_template.format(chain=chain, view_name=full_view_name)\n",
    "    \n",
    "    client.command(query)\n",
    "    print(f\"Created materialized view {full_view_name}\")\n",
    "\n",
    "def ensure_backfill_tracking_table_exists(client):\n",
    "    check_table_query = \"\"\"\n",
    "    SELECT 1 FROM system.tables \n",
    "    WHERE database = currentDatabase() AND name = 'backfill_tracking'\n",
    "    \"\"\"\n",
    "    result = client.query(check_table_query)\n",
    "    \n",
    "    if not result.result_rows:\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE backfill_tracking (\n",
    "            chain String,\n",
    "            mv_name String,\n",
    "            start_date Date,\n",
    "            end_date Date\n",
    "        ) ENGINE = MergeTree()\n",
    "        ORDER BY (chain, mv_name, start_date)\n",
    "        \"\"\"\n",
    "        client.command(create_table_query)\n",
    "        print(\"Created backfill_tracking table.\")\n",
    "    else:\n",
    "        print(\"backfill_tracking table already exists.\")\n",
    "\n",
    "def backfill_data(client, chain, mv_name):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    current_date = start_date\n",
    "    batch_size = datetime.timedelta(days=14)\n",
    "\n",
    "    while current_date < end_date:\n",
    "        batch_end = min(current_date + batch_size, end_date)\n",
    "        \n",
    "        # Check if this range has been backfilled\n",
    "        check_query = f\"\"\"\n",
    "        SELECT 1\n",
    "        FROM backfill_tracking\n",
    "        WHERE chain = '{chain}'\n",
    "          AND mv_name = '{mv_name}'\n",
    "          AND start_date <= toDate('{current_date}')\n",
    "          AND end_date >= toDate('{batch_end}')\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        result = client.query(check_query)\n",
    "        \n",
    "        if not result.result_rows:\n",
    "            # No record of backfill, proceed\n",
    "            query_template = get_query_from_file(f'{mv_name}_backfill')\n",
    "            query = query_template.format(\n",
    "                view_name=full_view_name,\n",
    "                chain=chain,\n",
    "                start_date=current_date,\n",
    "                end_date=batch_end\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                client.command(query)\n",
    "                \n",
    "                # Record the backfill\n",
    "                track_query = f\"\"\"\n",
    "                INSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\n",
    "                VALUES ('{chain}', '{mv_name}', toDate('{current_date}'), toDate('{batch_end}'))\n",
    "                \"\"\"\n",
    "                client.command(track_query)\n",
    "                \n",
    "                print(f\"Backfilled data for {full_view_name} from {current_date} to {batch_end}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during backfill for {full_view_name} from {current_date} to {batch_end}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Data already backfilled for {full_view_name} from {current_date} to {batch_end}. Skipping.\")\n",
    "        \n",
    "        current_date = batch_end + datetime.timedelta(days=1)\n",
    "\n",
    "def optimize_remove_dupes(client, chain, mv_name):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    # Optimize table to merge and remove duplicates\n",
    "    optimize_query = f\"OPTIMIZE TABLE {full_view_name} FINAL\"\n",
    "    client.command(optimize_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_materialized_view(client, chain, mv_name):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "\n",
    "    try:\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped materialized view {full_view_name}\")\n",
    "\n",
    "        # Recreate the materialized view using the existing function\n",
    "        create_materialized_view(client, chain, mv_name)\n",
    "        print(f\"Recreated materialized view {full_view_name}\")\n",
    "\n",
    "        # Clear the backfill tracking for this view\n",
    "        client.command(f\"\"\"\n",
    "        ALTER TABLE backfill_tracking \n",
    "        DELETE WHERE chain = '{chain}' AND mv_name = '{mv_name}'\n",
    "        \"\"\")\n",
    "        print(f\"Cleared backfill tracking for {full_view_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error resetting materialized view {full_view_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To reset a view\n",
    "# reset_materialized_view(client, 'op', 'erc20_transfers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "ensure_backfill_tracking_table_exists(client)\n",
    "\n",
    "for chain in chains:\n",
    "    print(f\"Processing chain: {chain}\")\n",
    "    for mv_name in mv_names:\n",
    "        create_materialized_view(client, chain, mv_name)\n",
    "        backfill_data(client, chain, mv_name)\n",
    "        optimize_remove_dupes(client, chain, mv_name)\n",
    "    print(f\"Completed processing for {chain}\")\n",
    "\n",
    "print(\"All chains and views processed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
