{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of materialized view names\n",
    "mv_names = [\n",
    "        # in order of build\n",
    "        'erc20_transfers',\n",
    "        'native_eth_transfers',\n",
    "        'transactions_unique',\n",
    "\n",
    "        'daily_aggregate_transactions_to',\n",
    "        'daily_aggregate_chain_stats', #comment out, if we can't align datatypes across chains.\n",
    "        ]\n",
    "set_days_batch_size = 7\n",
    "\n",
    "optimize_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "import opstack_metadata_utils as ops\n",
    "import goldsky_db_utils as gsb\n",
    "sys.path.pop()\n",
    "client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chain_name</th>\n",
       "      <th>display_name</th>\n",
       "      <th>mainnet_chain_id</th>\n",
       "      <th>chain_layer</th>\n",
       "      <th>alignment</th>\n",
       "      <th>da_layer</th>\n",
       "      <th>output_root_layer</th>\n",
       "      <th>gas_token</th>\n",
       "      <th>block_time_sec</th>\n",
       "      <th>blockchain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>op</td>\n",
       "      <td>OP Mainnet</td>\n",
       "      <td>10.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>op</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zora</td>\n",
       "      <td>Zora</td>\n",
       "      <td>7777777.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>zora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>base</td>\n",
       "      <td>Base</td>\n",
       "      <td>8453.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mode</td>\n",
       "      <td>Mode</td>\n",
       "      <td>34443.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lisk</td>\n",
       "      <td>Lisk</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>lisk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>metal</td>\n",
       "      <td>Metal</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mint</td>\n",
       "      <td>Mint</td>\n",
       "      <td>185.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>mint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fraxtal</td>\n",
       "      <td>Fraxtal</td>\n",
       "      <td>252.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>fraxtalda</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>frxETH</td>\n",
       "      <td>2</td>\n",
       "      <td>fraxtal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>redstone</td>\n",
       "      <td>Redstone</td>\n",
       "      <td>690.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>op-plasma</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>redstone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cyber</td>\n",
       "      <td>Cyber</td>\n",
       "      <td>7560.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>eigenda</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>cyber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>swan</td>\n",
       "      <td>SwanChain</td>\n",
       "      <td>254.0</td>\n",
       "      <td>L2</td>\n",
       "      <td>OP Chain</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ethereum</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2</td>\n",
       "      <td>swan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chain_name display_name  mainnet_chain_id chain_layer alignment   da_layer  \\\n",
       "0          op   OP Mainnet              10.0          L2  OP Chain   ethereum   \n",
       "1        zora         Zora         7777777.0          L2  OP Chain   ethereum   \n",
       "2        base         Base            8453.0          L2  OP Chain   ethereum   \n",
       "3        mode         Mode           34443.0          L2  OP Chain   ethereum   \n",
       "5        lisk         Lisk            1135.0          L2  OP Chain   ethereum   \n",
       "6       metal        Metal            1750.0          L2  OP Chain   ethereum   \n",
       "7        mint         Mint             185.0          L2  OP Chain   ethereum   \n",
       "13    fraxtal      Fraxtal             252.0          L2  OP Chain  fraxtalda   \n",
       "14   redstone     Redstone             690.0          L2  OP Chain  op-plasma   \n",
       "15      cyber        Cyber            7560.0          L2  OP Chain    eigenda   \n",
       "39       swan    SwanChain             254.0          L2  OP Chain   ethereum   \n",
       "\n",
       "   output_root_layer gas_token  block_time_sec blockchain  \n",
       "0           ethereum       ETH               2         op  \n",
       "1           ethereum       ETH               2       zora  \n",
       "2           ethereum       ETH               2       base  \n",
       "3           ethereum       ETH               2       mode  \n",
       "5           ethereum       ETH               2       lisk  \n",
       "6           ethereum       ETH               2      metal  \n",
       "7           ethereum       ETH               2       mint  \n",
       "13          ethereum    frxETH               2    fraxtal  \n",
       "14          ethereum       ETH               2   redstone  \n",
       "15          ethereum       ETH               2      cyber  \n",
       "39          ethereum       ETH               2       swan  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Chain List\n",
    "chain_configs = ops.get_superchain_metadata_by_data_source('oplabs') # OPLabs db\n",
    "\n",
    "if client is None:\n",
    "        client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "# Function to create ClickHouse view\n",
    "def get_chain_names_from_df(df):\n",
    "    return df['blockchain'].dropna().unique().tolist()\n",
    "\n",
    "chain_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of chains\n",
    "# chains = get_chain_names_from_df(chain_configs)\n",
    "\n",
    "# Start date for backfilling\n",
    "start_date = datetime.date(2021, 11, 1)\n",
    "end_date = datetime.date.today() + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_from_file(mv_name):\n",
    "    try:\n",
    "        # Try to get the directory of the current script\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # If __file__ is not defined (e.g., in Jupyter), use the current working directory\n",
    "        script_dir = os.getcwd()\n",
    "    \n",
    "    query_file_path = os.path.join(script_dir, 'mv_inputs', f'{mv_name}.sql')\n",
    "    print(f\"Attempting to read query from: {query_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(query_file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Query file not found: {query_file_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimize_on_insert(option_int = 1):\n",
    "    client.command(f\"\"\"\n",
    "        SET optimize_on_insert = {option_int};\n",
    "        \"\"\")\n",
    "    print(f\"Set optimize_on_insert = {option_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_materialized_view(client, chain, mv_name, block_time = 2):\n",
    "    table_view_name = f'{chain}_{mv_name}'\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    create_file_name = f'{mv_name}_create'\n",
    "    print(full_view_name)\n",
    "    \n",
    "    # Check if create file exists\n",
    "    if not os.path.exists(f'mv_inputs/{create_file_name}.sql'):\n",
    "        print(f\"Create file {create_file_name}.sql does not exist. Skipping creation.\")\n",
    "    else:\n",
    "        # Check if table already exists\n",
    "        result = client.query(f\"SHOW TABLES LIKE '{table_view_name}'\")\n",
    "        if result.result_rows:\n",
    "            print(f\"Table {table_view_name} already exists. Skipping creation.\")\n",
    "        else:\n",
    "            # Create the table\n",
    "            create_query = get_query_from_file(create_file_name)\n",
    "            create_query = create_query.format(chain=chain, view_name=table_view_name)\n",
    "            client.command(create_query)\n",
    "            print(f\"Created table {table_view_name}\")\n",
    "\n",
    "    # Check if view already exists\n",
    "    result = client.query(f\"SHOW TABLES LIKE '{full_view_name}'\")\n",
    "    if result.result_rows:\n",
    "        print(f\"Materialized view {full_view_name} already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    query_template = get_query_from_file(f'{mv_name}_mv')\n",
    "    query = query_template.format(chain=chain, view_name=full_view_name, table_name = table_view_name, block_time_sec = block_time)\n",
    "    query = gsb.process_goldsky_sql(query)\n",
    "    \n",
    "    # print(query)\n",
    "    \n",
    "    client.command(query)\n",
    "    print(f\"Created materialized view {full_view_name}\")\n",
    "\n",
    "def ensure_backfill_tracking_table_exists(client):\n",
    "    check_table_query = \"\"\"\n",
    "    SELECT 1 FROM system.tables \n",
    "    WHERE database = currentDatabase() AND name = 'backfill_tracking'\n",
    "    \"\"\"\n",
    "    result = client.query(check_table_query)\n",
    "    \n",
    "    if not result.result_rows:\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE backfill_tracking (\n",
    "            chain String,\n",
    "            mv_name String,\n",
    "            start_date Date,\n",
    "            end_date Date\n",
    "        ) ENGINE = MergeTree()\n",
    "        ORDER BY (chain, mv_name, start_date)\n",
    "        \"\"\"\n",
    "        client.command(create_table_query)\n",
    "        print(\"Created backfill_tracking table.\")\n",
    "    else:\n",
    "        print(\"backfill_tracking table already exists.\")\n",
    "\n",
    "def backfill_data(client, chain, mv_name, block_time = 2):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    full_table_name = f'{chain}_{mv_name}'\n",
    "    current_date_q = f\"SELECT DATE_TRUNC('day',MIN(timestamp)) AS start_dt FROM {chain}_blocks WHERE number = 1 AND is_deleted = 0\"\n",
    "    current_date = client.query(current_date_q).result_rows[0][0].date()\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        attempts = 1\n",
    "        is_success = 0\n",
    "        days_batch_size = set_days_batch_size\n",
    "        \n",
    "        while is_success == 0 & attempts < 3:\n",
    "            batch_size = datetime.timedelta(days=days_batch_size)\n",
    "\n",
    "            batch_end = min(current_date + batch_size, end_date)\n",
    "            \n",
    "            # Check if this range has been backfilled\n",
    "            check_query = f\"\"\"\n",
    "            SELECT 1\n",
    "            FROM backfill_tracking\n",
    "            WHERE chain = '{chain}'\n",
    "            AND mv_name = '{mv_name}'\n",
    "            HAVING \n",
    "                MIN(start_date) <= toDate('{current_date}')\n",
    "            AND MAX(end_date) >= toDate('{batch_end}')\n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "            result = client.query(check_query)\n",
    "            # print(check_query)\n",
    "            # print(result.result_rows)\n",
    "            #Check if data already exists\n",
    "            \n",
    "\n",
    "\n",
    "            if not result.result_rows:\n",
    "                # No record of backfill, proceed\n",
    "                query_template = get_query_from_file(f'{mv_name}_backfill')\n",
    "                query = query_template.format(\n",
    "                    view_name=full_view_name,\n",
    "                    chain=chain,\n",
    "                    start_date=current_date,\n",
    "                    end_date=batch_end,\n",
    "                    table_name = full_table_name,\n",
    "                    block_time_sec = block_time\n",
    "                )\n",
    "                query = gsb.process_goldsky_sql(query)\n",
    "                \n",
    "                # print(query)\n",
    "                try:\n",
    "                    # print(query)\n",
    "                    set_optimize_on_insert()\n",
    "                    client.command(query)\n",
    "                    # Record the backfill\n",
    "                    track_query = f\"\"\"\n",
    "                    INSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\n",
    "                    VALUES ('{chain}', '{mv_name}', toDate('{current_date}'), toDate('{batch_end}'))\n",
    "                    \"\"\"\n",
    "                    client.command(track_query)\n",
    "                    \n",
    "                    print(f\"Backfilled data for {full_view_name} from {current_date} to {batch_end}\")\n",
    "\n",
    "                    # Optimize the newly backfilled partition\n",
    "                    # optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "                    is_success = 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during backfill for {full_view_name} from {current_date} to {batch_end}: {str(e)}\")\n",
    "                    days_batch_size = 1\n",
    "                    attempts += 1\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print(f\"Data already backfilled for {full_view_name} from {current_date} to {batch_end}. Skipping.\")\n",
    "                is_success = 1\n",
    "                # if optimize_all:\n",
    "                #     optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "\n",
    "            current_date = batch_end + datetime.timedelta(days=1)\n",
    "\n",
    "# def optimize_partition(client, full_view_name, start_date, end_date):\n",
    "#     # First, let's get the actual partition names\n",
    "#     partition_query = f\"\"\"\n",
    "#     SELECT DISTINCT partition\n",
    "#     FROM system.parts\n",
    "#     WHERE table = '{full_view_name.split('.')[-1]}'\n",
    "#       AND database = '{full_view_name.split('.')[0]}'\n",
    "#       AND active\n",
    "#     ORDER BY partition\n",
    "#     \"\"\"\n",
    "#     print(partition_query)\n",
    "#     partitions = [row[0] for row in client.query(partition_query).result_rows]\n",
    "    \n",
    "#     print(f\"Available partitions for {full_view_name}: {partitions}\")\n",
    "\n",
    "#     # Filter partitions within our date range\n",
    "#     start_partition = start_date.strftime('%Y%m')\n",
    "#     end_partition = end_date.strftime('%Y%m')\n",
    "#     partitions_to_optimize = [p for p in partitions if start_partition <= p <= end_partition]\n",
    "\n",
    "#     if not partitions_to_optimize:\n",
    "#         print(f\"No partitions found for {full_view_name} between {start_date} and {end_date}\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         for partition in partitions_to_optimize:\n",
    "#             optimize_query = f\"\"\"\n",
    "#             OPTIMIZE TABLE {full_view_name} \n",
    "#             PARTITION '{partition}'\n",
    "#             FINAL SETTINGS max_execution_time = 3000\n",
    "#             \"\"\"\n",
    "#             print(f\"Attempting to optimize {full_view_name} for partition {partition}\")\n",
    "#             client.command(optimize_query)\n",
    "#             print(f\"Successfully optimized partition {partition} for {full_view_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing OPTIMIZE TABLE for {full_view_name}\")\n",
    "#         print(f\"  Partition: {partition}\")\n",
    "#         print(f\"  Date range: {start_date} to {end_date}\")\n",
    "#         print(f\"  Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_materialized_view(client, chain, mv_name, block_time = 2):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    table_name = f'{chain}_{mv_name}'\n",
    "\n",
    "    try:\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        # client.command(f\"DROP MATERIALIZED VIEW IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped materialized view {full_view_name}\")\n",
    "\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        print(f\"Dropped table {table_name}\")\n",
    "\n",
    "        # # Recreate the materialized view using the existing function\n",
    "        # create_materialized_view(client, chain, mv_name, block_time)\n",
    "        # print(f\"Recreated materialized view {full_view_name}\")\n",
    "\n",
    "        # Clear the backfill tracking for this view\n",
    "        client.command(f\"\"\"\n",
    "        ALTER TABLE backfill_tracking \n",
    "        DELETE WHERE chain = '{chain}' AND mv_name = '{mv_name}'\n",
    "        \"\"\")\n",
    "        print(f\"Cleared backfill tracking for {full_view_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error resetting materialized view {full_view_name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped materialized view op_daily_aggregate_transactions_mv\n",
      "Dropped table op_daily_aggregate_transactions\n",
      "Cleared backfill tracking for op_daily_aggregate_transactions_mv\n",
      "Dropped materialized view zora_daily_aggregate_transactions_mv\n",
      "Dropped table zora_daily_aggregate_transactions\n",
      "Cleared backfill tracking for zora_daily_aggregate_transactions_mv\n",
      "Dropped materialized view base_daily_aggregate_transactions_mv\n",
      "Dropped table base_daily_aggregate_transactions\n",
      "Cleared backfill tracking for base_daily_aggregate_transactions_mv\n",
      "Dropped materialized view mode_daily_aggregate_transactions_mv\n",
      "Dropped table mode_daily_aggregate_transactions\n",
      "Cleared backfill tracking for mode_daily_aggregate_transactions_mv\n",
      "Dropped materialized view lisk_daily_aggregate_transactions_mv\n",
      "Dropped table lisk_daily_aggregate_transactions\n",
      "Cleared backfill tracking for lisk_daily_aggregate_transactions_mv\n",
      "Dropped materialized view metal_daily_aggregate_transactions_mv\n",
      "Dropped table metal_daily_aggregate_transactions\n",
      "Cleared backfill tracking for metal_daily_aggregate_transactions_mv\n",
      "Dropped materialized view mint_daily_aggregate_transactions_mv\n",
      "Dropped table mint_daily_aggregate_transactions\n",
      "Cleared backfill tracking for mint_daily_aggregate_transactions_mv\n",
      "Dropped materialized view fraxtal_daily_aggregate_transactions_mv\n",
      "Dropped table fraxtal_daily_aggregate_transactions\n",
      "Cleared backfill tracking for fraxtal_daily_aggregate_transactions_mv\n",
      "Dropped materialized view redstone_daily_aggregate_transactions_mv\n",
      "Dropped table redstone_daily_aggregate_transactions\n",
      "Cleared backfill tracking for redstone_daily_aggregate_transactions_mv\n",
      "Dropped materialized view cyber_daily_aggregate_transactions_mv\n",
      "Dropped table cyber_daily_aggregate_transactions\n",
      "Cleared backfill tracking for cyber_daily_aggregate_transactions_mv\n",
      "Dropped materialized view swan_daily_aggregate_transactions_mv\n",
      "Dropped table swan_daily_aggregate_transactions\n",
      "Cleared backfill tracking for swan_daily_aggregate_transactions_mv\n"
     ]
    }
   ],
   "source": [
    "# # # # # To reset a view\n",
    "# for row in chain_configs.itertuples(index=False):\n",
    "#         chain = row.chain_name\n",
    "#         reset_materialized_view(client, chain, 'daily_aggregate_transactions', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "ensure_backfill_tracking_table_exists(client)\n",
    "\n",
    "for row in chain_configs.itertuples(index=False):\n",
    "    chain = row.chain_name\n",
    "    block_time = row.block_time_sec\n",
    "    print(f\"Processing chain: {chain}\")\n",
    "    for mv_name in mv_names:\n",
    "        try:\n",
    "            print('create matview')\n",
    "            create_materialized_view(client, chain, mv_name, block_time)\n",
    "        except:\n",
    "            print('error')\n",
    "        try:\n",
    "            print('create backfill')\n",
    "            backfill_data(client, chain, mv_name, block_time)\n",
    "        except:\n",
    "            print('error')\n",
    "        \n",
    "    print(f\"Completed processing for {chain}\")\n",
    "\n",
    "print(\"All chains and views processed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
